---
date created: 2021-11-13
---


# 第二次

## 1. hadoop 配置

### 1.1. 将压缩包上传至虚拟机

### 1.2. 解压 hadoop-2.6.0-cdh5.7.0.tar

```shell
$ tar xvf hadoop-2.6.0-cdh5.7.0.tar -C ~/hadoop
```

### 1.3. 编辑 Hadoop 环境配置文件

```shell
$ vi ~/hadoop/etc/hadoop/hadoop-env.sh
```

```bash
export JAVA_HOME=/usr/java/jdk1.8
```

![[public/shixun/Pasted_image_20211113191228.png]]

### 1.4. 配置 Hadoop 环境变量

使用 vim 编辑 `.bash_profile`

```shell
$ vi ~/.bash_profile
```

```bash
export JAVA_HOME=/usr/java/jdk1.8
export HADOOP_HOME=/home/lc/hadoop
PATH=$PATH:$JAVA_HOME/bin
PATH=$PATH:$HADOOP_HOME/bin
PATH=$PATH:$HADOOP_HOME/sbin
export PATH
```

执行命令重载

```shell
$ source ~/.bash_profile
```

通过 `hadoop version` 命令可以查看版本号。

![[public/shixun/Pasted_image_20211113194503.png]]

### 1.5. 添加 hostname

修改 `/etc/hosts` 文件，给虚拟机添加主机名

```shell
$ sudo vi /etc/hosts
```

在 hosts 末尾添加

```text
172.22.0.3 master  # 主机 IP 地址
172.22.0.4 slave1  # 从属机1 IP 地址
172.22.0.5 slave2  # 从属机2 IP 地址
172.22.0.6 slave3  # 从属机3 IP 地址
```

重新登录主机后，主机用户提示为 `[lc@master ~]`，而以前为 `[lc@localhost ~]`。

### 1.6. 配置 Hadoop 核心文件

```shell
$ vi hadoop/etc/hadoop/core-site.xml
```

```xml
<configuration>
  <property>
	  <name>fs.defaultFS</name>
	  <value>hdfs://master:8020</value>
  </property>
  <property>
    <name>hadoop.tmp.dir</name>
	<value>file:///home/lc/hadoop/tmp</value>
	  <description>Abase for other temporary diretories.</description>
  </property>
</configuration>
```

### 1.7. 配置 HDFS 文件

```shell
$ vi hadoop/etc/hadoop/hdfs-site.xml
```

```xml
<configuration>
  <property>
    <name>dfs.namenode.name.dir</name>
	<value>file:///home/lc/hadoop/hdfs/name</value>
	<!-- 注：pdf上为 file:///home/lc/hadoop/tmp/hdfs/name -->
  </property>
  <property>
	<name>dfs.replication</name>
	<value>1</value>
  </property>
</configuration>
```

^hadoop-hdfs-dir

## 2. 启动与测试

### 2.1. 创建 tmp 文件夹

在 `~/hadoop/` 下新建 `tmp/` 目录，用于存放生成的文件。

```shell
$ pwd
/home/lc/hadoop
$ mkdir tmp
```

### 2.2. 执行初始化 NameNode 命令

```shell
$ hdfs namenode -format
```

如下图，出现 `successfully formatted` 则初始化成功。

![[public/shixun/Pasted_image_20211113104000.png]]

### 2.3. 启动 Hadoop

执行 `start-dfs.sh` 命令，之后执行 `jps`，可以看到 ==NameNode==，则运行成功。

```shell
$ start-dfs.sh
$ jps
```

![[public/shixun/Pasted_image_20211113114547.png]]

### 2.3+ 配置 SSH 密钥

在启动过程中需要多次输入用户密码，停止同样需要，可以配置 SSH 免输入密码。

#### 产生密钥

在 master 主机终端用户主目录执行

```shell
$ ssh-keygen -t rsa
```

一路回车，得到生成的公钥和私钥。此处已有密钥，不再覆盖。

![[public/shixun/Pasted_image_20211113200643.png]]

#### 复制公钥

使用命令，复制一份公钥，作为验证公钥

```shell
$ cp id_rsa_pub authorized_keys
```

#### 下载私钥

将私钥下载到实体机，放到合适的位置

![[public/shixun/Pasted_image_20211113195842.png]]

#### 在 Mobaxterm 设置使用私钥

![[public/shixun/Pasted_image_20211113200019.png]]

### 2.4. 设置防火墙允许访问 50070 端口

```shell
$ sudo firewall-cmd --add-port=50070/tcp --zone=public --permanent
$ sudo firewall-cmd --reload
$ sudo firewall-cmd --list-ports
```

![[public/shixun/Pasted_image_20211113114524.png]]

### 2.5. 在宿主机上查看

打开浏览器，进入虚拟机 IP 地址下的 50070 端口，即 `172.22.0.3:50070`

![[public/shixun/Pasted_image_20211113121325.png]]
